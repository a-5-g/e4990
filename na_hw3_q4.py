# -*- coding: utf-8 -*-
"""na_hw3_q4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OaqjfKmpzCmutJ4WbukDr58eocpVdwF0
"""

import numpy as np
from numpy import linalg as LA
from numpy.linalg import inv
from numpy.linalg import det

def norm(x):
  return LA.norm(x,2)

def is_pos_def(x):
    return np.all(LA.eigvals(x) > 0)
  
def f1(x):
  return -13 + x[0] + ((5 - x[1]) * x[1] - 2) * x[1]

def f2(x):
  return -29 + x[0] + ((x[1] + 1) * x[1] - 14) * x[1]

def f(x):
  return f1(x)*f1(x) + f2(x)*f2(x)

def gradf1(x):
  g1f1 = 1
  g2f1 = -3*pow(x[1],2) + 10*x[1] - 2
  return np.array([g1f1, g2f1], dtype=np.float64)

def gradf2(x):
  g1f2 = 1
  g2f2 = 3*pow(x[1],2) + 2*x[1] - 14
  return np.array([g1f2, g2f2], dtype=np.float64)

def gradf(x):
  return 2*f1(x)*gradf1(x) + 2*f2(x)*gradf2(x)

def h(x):
  h1 = 4
  h2 = 24*x[1] - 32
  h3 = h2
  h4 = 60*pow(x[1],4) - 160*pow(x[1],3) + 24*pow(x[1],2) - 480*x[1] + 24*x[0] + 24
  return np.array([[h1, h2], [h3,h4]], dtype=np.float64)

def J(x):
  return np.array([[gradf1(x)[0], gradf1(x)[1]], [gradf2(x)[0], gradf2(x)[1]]], dtype=np.float64)

def F(x):
  return np.array([f1(x), f2(x)], dtype=np.float64)

def backtracking(x,alpha,beta,s,epsilon):
  iter = 0
  while(norm(gradf(x)) > epsilon):
    iter = iter + 1
    t = s
    v1 = f(x) - f(x - t*gradf(x))
    v2 = alpha * t * norm(gradf(x))**2
    while(v1 < v2):
      t = beta * t
      v1 = f(x) - f(x - t*gradf(x))
      v2 = alpha * t * norm(gradf(x))**2
    x = x - t * gradf(x)
  print("The method converged")
  return iter, x, f(x)

def newton_hybrid(x,alpha,beta,s,epsilon):
  iter = 0
  d = -gradf(x)

  if is_pos_def(h(x)):
    d = LA.solve(h(x), -gradf(x))
  
  while(norm(gradf(x)) > epsilon):
    d = -gradf(x)

    if is_pos_def(h(x)):
      d = LA.solve(h(x), -gradf(x))

    iter = iter + 1
    t = s
    v1 = f(x) - f(x + t*d)
    v2 = -alpha * t * (gradf(x).T @ d)
    while(v1 < v2):
      t = beta * t
      v1 = f(x) - f(x + t*d)
      v2 = -alpha * t * (gradf(x).T @ d)
    x = x + t * d
  print("The method converged")
  return iter, x, f(x)

def damped_gauss_newton(x,alpha,beta,s,epsilon):
  iter = 0
  
  while(norm(gradf(x)) > epsilon):

    if(LA.det(J(x).T @ J(x)) == 0):
      print("J(x_k)^T * J(x_k) is singular")
      print("The method did not converge")
      print("The current values of number of iterations, x and f(x) are:")
      return iter, x, f(x)
      
    temp = inv(J(x).T @ J(x))
    d = (-temp @ J(x).T) @ F(x)
    iter = iter + 1
    t = s
    v1 = f(x) - f(x + t*d)
    v2 = -alpha * t * (gradf(x).T @ d)

    while(v1 < v2):
      t = beta * t
      v1 = f(x) - f(x + t*d)
      v2 = -alpha * t * (gradf(x).T @ d)
    
    x = x + t * d
  print("The method converged")
  return iter, x, f(x)

epsilon = 1e-5
s = 1
alpha = 0.5
beta = 0.5
x1 = np.array([-50,7], dtype=np.float64)
x2 = np.array([20,7], dtype=np.float64)
x3 = np.array([20,-18], dtype=np.float64)
x4 = np.array([5,-10], dtype=np.float64)

"""### For the gradient method with backtracking"""

print("For x0 =", x1)
iterb1, xb1, fb1 = backtracking(x1,alpha,beta,s,epsilon)
print("Number of iterations:", iterb1,", x:", xb1, ", f(x):",fb1,"\n")

print("For x0 =", x2)
iterb2, xb2, fb2 = backtracking(x2,alpha,beta,s,epsilon)
print("Number of iterations:", iterb2,", x:", xb2, ", f(x):",fb2,"\n")

print("For x0 =", x3)
iterb3, xb3, fb3 = backtracking(x3,alpha,beta,s,epsilon)
print("Number of iterations:", iterb3,", x:", xb3, ", f(x):",fb3,"\n")

print("For x0 =", x4)
iterb4, xb4, fb4 = backtracking(x4,alpha,beta,s,epsilon)
print("Number of iterations:", iterb4,", x:", xb4, ", f(x):",fb4,"\n")

"""### For the hybrid Newton’s method"""

print("For x0 =", x1)
iterb1, xb1, fb1 = newton_hybrid(x1,alpha,beta,s,epsilon)
print("Number of iterations:", iterb1,", x:", xb1, ", f(x):",fb1,"\n")

print("For x0 =", x2)
iterb2, xb2, fb2 = newton_hybrid(x2,alpha,beta,s,epsilon)
print("Number of iterations:", iterb2,", x:", xb2, ", f(x):",fb2,"\n")

print("For x0 =", x3)
iterb3, xb3, fb3 = newton_hybrid(x3,alpha,beta,s,epsilon)
print("Number of iterations:", iterb3,", x:", xb3, ", f(x):",fb3,"\n")

print("For x0 =", x4)
iterb4, xb4, fb4 = newton_hybrid(x4,alpha,beta,s,epsilon)
print("Number of iterations:", iterb4,", x:", xb4, ", f(x):",fb4,"\n")

"""### For the damped Gauss-Newton’s method with a backtracking line search strategy"""

print("For x0 =", x1)
iterb1, xb1, fb1 = damped_gauss_newton(x1,alpha,beta,s,epsilon)
print("Number of iterations:", iterb1,", x:", xb1, ", f(x):",fb1,"\n")

print("For x0 =", x2)
iterb2, xb2, fb2 = damped_gauss_newton(x2,alpha,beta,s,epsilon)
print("Number of iterations:", iterb2,", x:", xb2, ", f(x):",fb2,"\n")

print("For x0 =", x3)
iterb3, xb3, fb3 = damped_gauss_newton(x3,alpha,beta,s,epsilon)
print("Number of iterations:", iterb3,", x:", xb3, ", f(x):",fb3,"\n")

print("For x0 =", x4)
iterb4, xb4, fb4 = damped_gauss_newton(x4,alpha,beta,s,epsilon)
print("Number of iterations:", iterb4,", x:", xb4, ", f(x):",fb4,"\n")

